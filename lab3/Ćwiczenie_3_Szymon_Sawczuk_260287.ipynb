{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sieci neuronowe - ćwiczenie 3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Users/szymon/.pyenv/versions/3.10.6/lib/python3.10/site-packages (0.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(n: int) -> float:\n",
    "    return 1 / (1 + np.exp(-n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_der(n: int) -> float:\n",
    "    return sigmoid(n) * (1 - sigmoid(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerNetwork:\n",
    "   _weights: list[list[np.ndarray[float]]]\n",
    "   _biases: list[np.ndarray[float]] \n",
    "   _caches_x: list[list[np.ndarray[float]]] \n",
    "\n",
    "   def __cross_entropy_loss(self, y: np.ndarray, y_pred:np.ndarray) -> np.ndarray:\n",
    "      return np.sum(-y*np.log(y_pred) - (1 - y)*np.log(1 - y_pred)) / y_pred.shape[0]  \n",
    "   \n",
    "   def __cross_entropy_loss_der(self, y: np.ndarray, y_pred:np.ndarray) -> np.ndarray:\n",
    "      return np.sum(((1 - y) / (1 - y_pred)) - (y/y_pred)) \n",
    "\n",
    "   def __init__(self, hidden_layers_sizes = (1,) ):\n",
    "      self._weights = []\n",
    "      self._biases = []\n",
    "      self._caches_x = []\n",
    "\n",
    "      for index, layer_size in enumerate(hidden_layers_sizes):\n",
    "         self._weights.append([])\n",
    "         self._biases.append([])\n",
    "         self._caches_x.append([])\n",
    "         for _ in range(layer_size):\n",
    "            self._weights[index].append([])\n",
    "            self._caches_x[index].append([])\n",
    "      \n",
    "      # output layer\n",
    "      self._weights.append([])\n",
    "      self._caches_x.append([])\n",
    "      self._biases.append([])\n",
    "      self._weights[-1].append([])\n",
    "      self._caches_x[-1].append([])\n",
    "   \n",
    "   def __init_weights(self, size: int) -> np.ndarray:\n",
    "        return np.random.rand(size, 1) # initialize weights randomly\n",
    "   \n",
    "   def __optimize(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray, batch_size: int, learning_rate: float, \n",
    "                       min_step: float, max_iter: int) -> (list, list, list, list, list, list, list, list):\n",
    "      losses = []\n",
    "      weights = []\n",
    "      biases = []\n",
    "      losses_test = []\n",
    "\n",
    "      accuracy = []\n",
    "      f_score = []\n",
    "      precision = []\n",
    "      recalls = []\n",
    "\n",
    "      if batch_size > x_train.shape[0]:\n",
    "         batch_size = x_train.shape[0]\n",
    "\n",
    "      for index in range(max_iter): #learn for max_iter      \n",
    "         old_train_loss = self.__cross_entropy_loss(self._weights, x_train, y_train, self._bias)\n",
    "\n",
    "         shuffle = np.random.permutation(x_train.shape[0])\n",
    "         x_train_shuffled = x_train[shuffle]\n",
    "         y_train_shuffled = y_train[shuffle]\n",
    "\n",
    "         for batch_start_index in range(0, x_train.shape[0], batch_size):\n",
    "            x_train_batch = x_train_shuffled[batch_start_index:batch_start_index+batch_size] \n",
    "            y_train_batch = y_train_shuffled[batch_start_index:batch_start_index+batch_size] \n",
    "\n",
    "            pred = self.predict(x_train_batch)\n",
    "            errors = self.backward(y_train_batch, pred)\n",
    "\n",
    "            for layer_index, layer in enumerate(self._weights):\n",
    "               for neuron_index, _ in enumerate(layer):\n",
    "                  self._weights[layer_index][neuron_index] = self._weights[layer_index][neuron_index]  - learning_rate * np.dot(self._caches_x[layer_index][neuron_index], errors[layer_index][neuron_index])\n",
    "                  self._biases[layer_index][neuron_index] = self._biases[layer_index][neuron_index] - learning_rate * np.sum(errors[layer_index][neuron_index])\n",
    "                  \n",
    "\n",
    "         #calculate loss of training and testing data\n",
    "         new_train_loss = self.__cross_entropy_loss(y_train_batch, pred)\n",
    "         new_test_loss = self.__cross_entropy_loss(self._weights, x_test, y_test, self._bias)\n",
    "\n",
    "         #append to helper lists\n",
    "         losses.append(new_train_loss)\n",
    "         weights.append(self._weights)\n",
    "         biases.append(self._bias)\n",
    "         losses_test.append(new_test_loss)\n",
    "\n",
    "         #calculate scores for each iteration\n",
    "         y_pred = self.predict(x_test)\n",
    "         accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "         f_score.append(metrics.f1_score(y_test, y_pred))\n",
    "         precision.append(metrics.precision_score(y_test, y_pred))\n",
    "         recalls.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "         if min_step is not None and index > 0 and abs(old_train_loss - new_train_loss) <= min_step: #if change of loss is smaller or equal than min_step when stop learning\n",
    "               break\n",
    "\n",
    "      return losses, weights, biases, losses_test, accuracy, f_score, precision, recalls\n",
    "\n",
    "   def logarithmicRegression(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray, batch_size: int, learning_rate: float, \n",
    "                       max_iter: int, min_step: float = None) -> (list, list, list, list, list, list, list, list): #model convergence criteria are min_step (loss step) and max_iter (amount of iterations)\n",
    "      \n",
    "      curr_size_layer = x_train.shape[1]\n",
    "      \n",
    "      for layer_index, layer in enumerate(self._weights):\n",
    "         for neuron_index, neuron in enumerate(layer):\n",
    "            self._weights[layer_index][neuron_index] = self.__init_weights(curr_size_layer)\n",
    "         curr_size_layer = len(layer)\n",
    "         self._biases[layer_index] = self.__init_weights(curr_size_layer) #initialize bias randomly\n",
    "      \n",
    "      #fix shape of y data to match further calculations\n",
    "      if len(y_test.shape) == 1:\n",
    "         y_test = y_test[np.newaxis].T \n",
    "\n",
    "      if len(y_train.shape) == 1:\n",
    "         y_train = y_train[np.newaxis].T\n",
    "\n",
    "      losses, weights, biases, losses_test, accuracy, f_score, precision, recalls = self.__optimize(x_train, y_train, x_test, y_test, batch_size, learning_rate, min_step, max_iter) #optimize weights and bias using training data\n",
    "\n",
    "      #plot results\n",
    "      plt.plot(np.arange(len(losses)), losses, label=\"Train Loss\")\n",
    "      plt.plot(np.arange(len(losses)), losses_test, label=\"Test loss\")\n",
    "      plt.plot(np.arange(len(losses)), accuracy, label=\"Accuracy\")\n",
    "      plt.plot(np.arange(len(losses)), f_score, label=\"F_score\")\n",
    "      plt.plot(np.arange(len(losses)), precision, label=\"Precision\")\n",
    "      plt.plot(np.arange(len(losses)), recalls, label=\"Recall\")\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "      #print final weight, bias, loss and scores\n",
    "      print(\"Weights: \", self._weights)\n",
    "      print(\"Bias: \", self._bias)\n",
    "      print(\"Train loss: \", losses[-1])\n",
    "      print(\"Test loss: \", losses_test[-1])\n",
    "\n",
    "      print(\"Scores\")\n",
    "      print(\"Accuracy: \", accuracy[-1])\n",
    "      print(\"F_score: \", f_score[-1])\n",
    "      print(\"Precision: \", precision[-1])\n",
    "      print(\"Recall: \", recalls[-1])\n",
    "\n",
    "      return losses, weights, biases, losses_test, accuracy, f_score, precision, recalls      \n",
    "      \n",
    "\n",
    "   def forward(self, x: np.ndarray) -> float:\n",
    "      for layer_index, layer in enumerate(self._weights):\n",
    "         for neuron_index, neuron in enumerate(layer):\n",
    "            self._caches_x[layer_index][neuron_index] = x\n",
    "         x = np.array([sigmoid(np.dot(x, neuron) + self._biases[layer_index][neuron_index]).flatten() for neuron in layer]).T\n",
    "      return x\n",
    "\n",
    "   def backward(self, y_train: np.ndarray, y_pred: np.ndarray) -> list[list[np.ndarray[float]]]:\n",
    "      errors: list[list[np.ndarray[float]]] = []\n",
    "      for layer_index, layer in enumerate(self._weights):\n",
    "         errors.append([])\n",
    "         for _ in layer:\n",
    "            errors[layer_index].append([])\n",
    "\n",
    "      err = self.__cross_entropy_loss_der(y_train, y_pred) * sigmoid_der(np.dot(self._caches_x[-1][0], self._weights[-1][0]) + self._biases[-1][0])\n",
    "      errors[-1][0] = err\n",
    "      next_weights = np.array(self._weights[-1]) # weights of last layer\n",
    "\n",
    "      for layer_index, layer in reversed(enumerate(self._weights[:-1])):\n",
    "         for neuron_index, _ in enumerate(layer):\n",
    "            err = np.dot(err, next_weights)\n",
    "            err = err * sigmoid_der(np.dot(self._caches_x[layer_index][neuron_index], self._weights[layer_index][neuron_index]) + self._biases[layer_index][neuron_index])\n",
    "            errors[layer_index][neuron_index] = err\n",
    "         next_weights = np.array(layer)\n",
    "         \n",
    "      return errors\n",
    "\n",
    "   def predict(self, x_test: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "      y_pred = self.forward(x_test) #calculate sigmoid for input data\n",
    "\n",
    "      #change posibility from sigmoid to 1 or 0\n",
    "      y_pred[y_pred >= 0.5] = 1\n",
    "      y_pred[y_pred < 0.5] = 0\n",
    "        \n",
    "      return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Przygotowanie danych na podstawie poprzedniego ćwiczenia</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code from https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "heart_data = heart_disease.data.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num\n",
      "0    164\n",
      "1    139\n",
      "Name: count, dtype: int64\n",
      "Index([166, 192, 287, 302], dtype='int64')\n",
      "num\n",
      "0    161\n",
      "1    138\n",
      "Name: count, dtype: int64\n",
      "Index([87, 264], dtype='int64')\n",
      "num\n",
      "0    160\n",
      "1    137\n",
      "Name: count, dtype: int64\n",
      "num\n",
      "1    137\n",
      "0    137\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>heart_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.378115</td>\n",
       "      <td>0.692333</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>1.586864</td>\n",
       "      <td>0.794188</td>\n",
       "      <td>-0.412694</td>\n",
       "      <td>1.027809</td>\n",
       "      <td>-1.766472</td>\n",
       "      <td>1.392528</td>\n",
       "      <td>0.354756</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>2.384985</td>\n",
       "      <td>-0.921154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.378115</td>\n",
       "      <td>0.692333</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>-0.659298</td>\n",
       "      <td>-0.366440</td>\n",
       "      <td>-0.412694</td>\n",
       "      <td>1.027809</td>\n",
       "      <td>-0.848538</td>\n",
       "      <td>1.392528</td>\n",
       "      <td>1.295154</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>1.337732</td>\n",
       "      <td>1.135027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.568371</td>\n",
       "      <td>-1.439120</td>\n",
       "      <td>-1.238467</td>\n",
       "      <td>-0.097757</td>\n",
       "      <td>-0.875487</td>\n",
       "      <td>-0.412694</td>\n",
       "      <td>1.027809</td>\n",
       "      <td>1.031040</td>\n",
       "      <td>-0.715498</td>\n",
       "      <td>0.269265</td>\n",
       "      <td>-1.015237</td>\n",
       "      <td>-0.756774</td>\n",
       "      <td>-0.921154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.131525</td>\n",
       "      <td>0.692333</td>\n",
       "      <td>-1.238467</td>\n",
       "      <td>-0.659298</td>\n",
       "      <td>-0.223907</td>\n",
       "      <td>-0.412694</td>\n",
       "      <td>-0.983760</td>\n",
       "      <td>1.293307</td>\n",
       "      <td>-0.715498</td>\n",
       "      <td>-0.243680</td>\n",
       "      <td>-1.015237</td>\n",
       "      <td>-0.756774</td>\n",
       "      <td>-0.921154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.811483</td>\n",
       "      <td>-1.439120</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>0.463783</td>\n",
       "      <td>0.427674</td>\n",
       "      <td>-0.412694</td>\n",
       "      <td>1.027809</td>\n",
       "      <td>0.506507</td>\n",
       "      <td>-0.715498</td>\n",
       "      <td>2.150062</td>\n",
       "      <td>2.276774</td>\n",
       "      <td>1.337732</td>\n",
       "      <td>-0.921154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.244851</td>\n",
       "      <td>-1.439120</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>0.463783</td>\n",
       "      <td>-0.122097</td>\n",
       "      <td>-0.412694</td>\n",
       "      <td>-0.983760</td>\n",
       "      <td>-1.110805</td>\n",
       "      <td>1.392528</td>\n",
       "      <td>-0.756625</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>-0.756774</td>\n",
       "      <td>1.135027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-1.115065</td>\n",
       "      <td>0.692333</td>\n",
       "      <td>-2.285812</td>\n",
       "      <td>-1.220838</td>\n",
       "      <td>0.346227</td>\n",
       "      <td>-0.412694</td>\n",
       "      <td>-0.983760</td>\n",
       "      <td>-0.717405</td>\n",
       "      <td>-0.715498</td>\n",
       "      <td>0.098283</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>-0.756774</td>\n",
       "      <td>1.135027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1.491441</td>\n",
       "      <td>0.692333</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>0.688399</td>\n",
       "      <td>-1.099468</td>\n",
       "      <td>2.414260</td>\n",
       "      <td>-0.983760</td>\n",
       "      <td>-0.324005</td>\n",
       "      <td>-0.715498</td>\n",
       "      <td>1.979081</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>1.337732</td>\n",
       "      <td>1.135027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.244851</td>\n",
       "      <td>0.692333</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>-0.097757</td>\n",
       "      <td>-2.361906</td>\n",
       "      <td>-0.412694</td>\n",
       "      <td>-0.983760</td>\n",
       "      <td>-1.460494</td>\n",
       "      <td>1.392528</td>\n",
       "      <td>0.098283</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.290479</td>\n",
       "      <td>1.135027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.244851</td>\n",
       "      <td>-1.439120</td>\n",
       "      <td>-1.238467</td>\n",
       "      <td>-0.097757</td>\n",
       "      <td>-0.223907</td>\n",
       "      <td>-0.412694</td>\n",
       "      <td>1.027809</td>\n",
       "      <td>1.118462</td>\n",
       "      <td>-0.715498</td>\n",
       "      <td>-0.927606</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.290479</td>\n",
       "      <td>-0.921154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>274 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex        cp  trestbps      chol       fbs   restecg  \\\n",
       "0    1.378115  0.692333  0.856224  1.586864  0.794188 -0.412694  1.027809   \n",
       "1    1.378115  0.692333  0.856224 -0.659298 -0.366440 -0.412694  1.027809   \n",
       "2   -1.568371 -1.439120 -1.238467 -0.097757 -0.875487 -0.412694  1.027809   \n",
       "3    0.131525  0.692333 -1.238467 -0.659298 -0.223907 -0.412694 -0.983760   \n",
       "4    0.811483 -1.439120  0.856224  0.463783  0.427674 -0.412694  1.027809   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "269  0.244851 -1.439120  0.856224  0.463783 -0.122097 -0.412694 -0.983760   \n",
       "270 -1.115065  0.692333 -2.285812 -1.220838  0.346227 -0.412694 -0.983760   \n",
       "271  1.491441  0.692333  0.856224  0.688399 -1.099468  2.414260 -0.983760   \n",
       "272  0.244851  0.692333  0.856224 -0.097757 -2.361906 -0.412694 -0.983760   \n",
       "273  0.244851 -1.439120 -1.238467 -0.097757 -0.223907 -0.412694  1.027809   \n",
       "\n",
       "      thalach     exang   oldpeak     slope        ca      thal  heart_disease  \n",
       "0   -1.766472  1.392528  0.354756  0.630769  2.384985 -0.921154              1  \n",
       "1   -0.848538  1.392528  1.295154  0.630769  1.337732  1.135027              1  \n",
       "2    1.031040 -0.715498  0.269265 -1.015237 -0.756774 -0.921154              0  \n",
       "3    1.293307 -0.715498 -0.243680 -1.015237 -0.756774 -0.921154              0  \n",
       "4    0.506507 -0.715498  2.150062  2.276774  1.337732 -0.921154              1  \n",
       "..        ...       ...       ...       ...       ...       ...            ...  \n",
       "269 -1.110805  1.392528 -0.756625  0.630769 -0.756774  1.135027              1  \n",
       "270 -0.717405 -0.715498  0.098283  0.630769 -0.756774  1.135027              1  \n",
       "271 -0.324005 -0.715498  1.979081  0.630769  1.337732  1.135027              1  \n",
       "272 -1.460494  1.392528  0.098283  0.630769  0.290479  1.135027              1  \n",
       "273  1.118462 -0.715498 -0.927606  0.630769  0.290479 -0.921154              1  \n",
       "\n",
       "[274 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df: pd.DataFrame = heart_data\n",
    "\n",
    "# repearing of the inbalnace in classification and removing null values\n",
    "df[\"num\"] = df[\"num\"].replace([2, 3, 4], 1) #change classes to binary classification\n",
    "print(df[\"num\"].value_counts())\n",
    "\n",
    "#get null values of ca and remove them\n",
    "null_idx = df[df[\"ca\"].isnull()].index \n",
    "print(null_idx)\n",
    "df = df.drop(null_idx)\n",
    "df = df.reset_index(drop=True) \n",
    "print(df[\"num\"].value_counts())\n",
    "\n",
    "#get null values of thel and remove them\n",
    "null_idx = df[df[\"thal\"].isnull()].index \n",
    "print(null_idx)\n",
    "df = df.drop(null_idx)\n",
    "df = df.reset_index(drop=True) \n",
    "print(df[\"num\"].value_counts())\n",
    "\n",
    "# balance classes to same amount 138\n",
    "random_idx = df.query(\"num == 0\").sample(df[\"num\"].value_counts()[0] - df[\"num\"].value_counts()[1]).index \n",
    "df = df.drop(random_idx)\n",
    "df = df.reset_index(drop=True)\n",
    "print(df[\"num\"].value_counts())\n",
    "\n",
    "df_without_num = df.loc[:, df.columns != \"num\"]\n",
    "std_features = (df_without_num - df_without_num.mean() )/ df_without_num.std() #(value-mean)/variance\n",
    "\n",
    "result = std_features\n",
    "result[\"heart_disease\"] = df[\"num\"]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(features, targets, percentage):\n",
    "    choices = np.random.choice(range(features.shape[0]), size=(int(features.shape[0] * percentage/100),), replace=False) \n",
    "    split = np.zeros(features.shape[0], dtype=bool)\n",
    "    split[choices] = True\n",
    "\n",
    "    return features[split], targets[split], features[~split], targets[~split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = result.loc[:, result.columns != \"heart_disease\"].to_numpy()\n",
    "targets = result[\"heart_disease\"].to_numpy()\n",
    "\n",
    "x_train, y_train, x_test, y_test = train_test_split(features, targets, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([-2.36165554, -1.43911965, -1.23846671, -0.77160578, -0.75331594,\n",
      "       -0.41269396, -0.98376047,  1.90526254, -0.71549768, -0.32917077,\n",
      "       -1.01523693, -0.75677419, -0.92115385]), array([-2.36165554, -1.43911965, -1.23846671, -0.77160578, -0.75331594,\n",
      "       -0.41269396, -0.98376047,  1.90526254, -0.71549768, -0.32917077,\n",
      "       -1.01523693, -0.75677419, -0.92115385]), array([-2.36165554, -1.43911965, -1.23846671, -0.77160578, -0.75331594,\n",
      "       -0.41269396, -0.98376047,  1.90526254, -0.71549768, -0.32917077,\n",
      "       -1.01523693, -0.75677419, -0.92115385])], [array([[0.01492239, 0.02781377, 0.01535772]]), array([[0.01492239, 0.02781377, 0.01535772]])], [array([[0.56563456, 0.56490518]])]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iter = 200\n",
    "\n",
    "ex1 = MultilayerNetwork((3,2))\n",
    "result_ex1 = ex1.logarithmicRegression(x_train, y_train, x_test, y_test, 0.2, max_iter, 0.0001)\n",
    "ex1.predict(x_test[63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191, 1)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle = np.random.permutation(y_train.shape[0])\n",
    "\n",
    "y_train[np.newaxis].T[shuffle].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
